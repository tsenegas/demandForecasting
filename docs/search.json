[
  {
    "objectID": "modeling_R.html",
    "href": "modeling_R.html",
    "title": "Demand Forecasting Model with R",
    "section": "",
    "text": "I chose to forecast at the SKU × Location level for several reasons:\n\nOperational relevance\n\nInventory and replenishment decisions are made at the store (location) and item (SKU) level. By forecasting precisely where and what to stock, we can minimize stock-outs and overstock.\n\nCaptures local demand patterns\n\nEach location has its own seasonality, promotions, and customer mix.\n\nSupports hierarchical consistency\n\nA bottom-up SKU×Location forecast can be aggregated automatically to Region or Family level views. That means our high-level dashboards always roll up correctly from the detailed forecasts driving operations.\n\nManages noise vs. scale\n\nWith only 3 SKUs and 10 locations (30 series total), it’s a manageable modeling effort. We get the benefit of detailed forecasts without an unmanageable number of models.\n\nFlexibility for enhancements\nWe can easily swap in covariates (e.g. price, promotions, local events) per SKU×Location, or test different algorithms (ARIMA, ETS, Prophet) on each series and still reconcile them back up.\n\nBy forecasting at the SKU × Location level, we ensure the most actionable, accurate, and consistent demand plans, while retaining the ability to roll up to any higher aggregation needed for reporting or strategic planning.\n\n1. Data Prepation\n\n\nCode\n# 0. Packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(modeltime)   # forecasting bridges tidymodels + prophet + ARIMA + ETS\nlibrary(timetk)      # time‐series toolkit\nlibrary(rsample)     # for resampling\nlibrary(parsnip)     # model specification\nlibrary(recipes)     # feature engineering\nlibrary(workflows)   # bundling recipe + model\nlibrary(yardstick)   # metrics\nlibrary(tsibble) \n\n# 1) Read & prep\ndf_monthly &lt;- read_csv(\"./data/df_monthly.csv\") |&gt;\n  select(month, Location = `Location ID`, SKU, quantity) |&gt;\n  group_by(month, Location, SKU) |&gt;\n  summarise(quantity = sum(quantity), .groups = \"drop\") |&gt;\n  mutate(month = yearmonth(month)) |&gt;\n  as_tsibble(key = c(Location, SKU), index = month) |&gt;\n  fill_gaps(quantity = 0L)\n\ndf &lt;- df_monthly |&gt;\n  select(month, Location, SKU, quantity) |&gt;\n  mutate(\n    ds = yearmonth(month) |&gt;  as.Date(),  # convert to Date at month‐start\n    y  = quantity\n  ) |&gt;\n  select(ds, y, Location, SKU)\n\n# 2) Nest by series\nnested_tbl &lt;- df |&gt; \n  group_by(Location, SKU) |&gt; \n  nest() |&gt; \n  ungroup()\n\n\n\n\n2. Model Training & Evaluation\n\n\nCode\nset.seed(123)\n\n# 3) Define a function to build a modeltime table per series\nmake_models &lt;- function(data_tbl) {\n\n  # a. Split train / test (last 6 months hold-out)\n  splits &lt;- initial_time_split(data_tbl, prop = (nrow(data_tbl) - 6) / nrow(data_tbl))\n\n  # b. Recipe (no extra features right now)\n  rec &lt;- recipe(y ~ ds, data = training(splits))\n  \n  # c. Model specs\n  arima_spec &lt;- arima_reg()      |&gt;  set_engine(\"auto_arima\")\n  ets_spec   &lt;- exp_smoothing()  |&gt;  set_engine(\"ets\")\n  prop_spec  &lt;- prophet_reg()    |&gt;  set_engine(\"prophet\")\n\n  # d. Workflows\n  wf_arima &lt;- workflow() |&gt;  add_model(arima_spec) |&gt;  add_recipe(rec)\n  wf_ets   &lt;- workflow() |&gt;  add_model(ets_spec)   |&gt;  add_recipe(rec)\n  wf_prop  &lt;- workflow() |&gt;  add_model(prop_spec)  |&gt;  add_recipe(rec)\n\n  # e. Modeltime table\n  modeltime_table(\n    arima = fit(wf_arima,  data = training(splits)),\n    ets   = fit(wf_ets,    data = training(splits)),\n    prop  = fit(wf_prop,   data = training(splits))\n  ) |&gt; \n    # f. Calibrate (generate forecasts on the test set)\n    modeltime_calibrate(new_data = testing(splits)) |&gt; \n    # g. Compute accuracy metrics\n    modeltime_accuracy() |&gt; \n    # h. Also embed the calibration object for later forecasting\n    left_join(\n      modeltime_table(\n        arima = fit(wf_arima,  data = training(splits)),\n        ets   = fit(wf_ets,    data = training(splits)),\n        prop  = fit(wf_prop,   data = training(splits))\n      ) |&gt;  modeltime_calibrate(new_data = testing(splits)) |&gt;  mutate(.model_desc = names(.model_id)),\n      by = \".model_id\"\n    )\n}\n\n# 4) Apply to each series\nresults_tbl &lt;- nested_tbl |&gt; \n  mutate(\n    metrics = map(data, make_models)\n  ) |&gt; \n  select(Location, SKU, metrics) |&gt; \n  unnest(metrics)\n\n# 5) Inspect accuracy\nbest_models &lt;- results_tbl |&gt; \n  group_by(Location, SKU) |&gt; \n  slice_min(rmse, n = 1) |&gt; \n  select(Location, SKU, .model_desc, mae, rmse, mase) |&gt; \n  distinct(Location, SKU, .keep_all = TRUE) \n\navg_month &lt;- df_monthly |&gt;\n  as.data.frame() |&gt;\n  group_by(Location, SKU) |&gt;\n  mutate(\n    avg_month = mean(quantity)\n  ) |&gt;\n  select(Location, SKU, avg_month) |&gt;\n  unique()\n\nbest_models &lt;- best_models |&gt;\n  left_join(\n    avg_month, by = c('Location', 'SKU')\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 2)))\n\nDT::datatable(best_models)\n\n\n\n\n\n\n\n\n3. Final Forecasting\n\n\nCode\n# 6) Re-nest for final forecasting\nforecast_tbl &lt;- nested_tbl |&gt; \n  left_join(\n    results_tbl |&gt; \n      group_by(Location, SKU) |&gt; \n      slice_min(rmse, n = 1) |&gt; \n      select(Location, SKU, .model_id),\n    by = c(\"Location\",\"SKU\")\n  ) |&gt; \n  distinct(Location, SKU, .keep_all = TRUE) |&gt; \n  mutate(\n    # 1) build a 3-row modeltime_table for each series, and tag rows with .model_id = 1,2,3\n    model_tbl = map(data, function(df) {\n      rec      &lt;- recipe(y ~ ds, data = df)\n      wf_arima &lt;- workflow() |&gt; \n                    add_model(arima_reg() |&gt;  set_engine(\"auto_arima\")) |&gt; \n                    add_recipe(rec)\n      wf_ets   &lt;- workflow() |&gt; \n                    add_model(exp_smoothing() |&gt;  set_engine(\"ets\")) |&gt; \n                    add_recipe(rec)\n      wf_prop  &lt;- workflow() |&gt; \n                    add_model(prophet_reg() |&gt;  set_engine(\"prophet\")) |&gt; \n                    add_recipe(rec)\n\n      modeltime_table(\n        arima = fit(wf_arima, df),\n        ets   = fit(wf_ets,   df),\n        prop  = fit(wf_prop,  df)\n      ) #|&gt; \n      #rowid_to_column(var = \".model_id\")\n    }),\n\n    # 2) pick the chosen row by .model_id\n    model = map2(model_tbl, .model_id, ~ slice(.x, .y)),\n\n    # 3) refit that model on _all_ the data, then forecast 6 months out\n    final_model = map2(model, data, ~ modeltime_refit(.x, data = .y))#,\n    #forecast    = map(final_model,     ~ modeltime_forecast(.x, h = \"6 months\"))\n  )\n\nforecast_tbl2 &lt;- forecast_tbl |&gt; \n  mutate(\n    # Re-calibrate each final_model on its own data\n    calib_full = map2(final_model, data, ~ modeltime_calibrate(.x, new_data = .y)),\n\n    # Now you *can* use `h = \"6 months\"`, but you MUST also give it `actual_data`\n    forecast = map2(\n      calib_full, data,\n      ~ modeltime_forecast(.x,\n          h           = \"6 months\",\n          actual_data = .y\n      )\n    )\n  )\n\n\n\nLocation 0001Location 0002Location 0003Location 0004Location 0005Location 0006Location 0007Location 0008Location 0009Location 0010\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_1 = forecast_tbl2 |&gt; \n  subset(Location == '0001' & SKU =='Alpha')\n \nalpha_1 = alpha_1$forecast |&gt;\n  as.data.frame()\n\nalpha_1_xts &lt;- xts(\n  x = alpha_1[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_1$.index\n)\n\ndygraph(alpha_1_xts, main = \"Forecast with 95% CI - Alpha*0001\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_1 = forecast_tbl2 |&gt; \n  subset(Location == '0001' & SKU =='Beta')\n \nbeta_1 = beta_1$forecast |&gt;\n  as.data.frame()\n\nbeta_1_xts &lt;- xts(\n  x = beta_1[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_1$.index\n)\n\ndygraph(beta_1_xts, main = \"Forecast with 95% CI - Beta*0001\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_1 = forecast_tbl2 |&gt; \n  subset(Location == '0001' & SKU =='Gamma')\n \ngamma_1 = gamma_1$forecast |&gt;\n  as.data.frame()\n\ngamma_1_xts &lt;- xts(\n  x = gamma_1[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_1$.index\n)\n\ndygraph(gamma_1_xts, main = \"Forecast with 95% CI - Gamma*0001\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_2 = forecast_tbl2 |&gt; \n  subset(Location == '0002' & SKU =='Alpha')\n \nalpha_2 = alpha_2$forecast |&gt;\n  as.data.frame()\n\nalpha_2_xts &lt;- xts(\n  x = alpha_2[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_2$.index\n)\n\ndygraph(alpha_2_xts, main = \"Forecast with 95% CI - Alpha*0002\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_2 = forecast_tbl2 |&gt; \n  subset(Location == '0002' & SKU =='Beta')\n \nbeta_2 = beta_2$forecast |&gt;\n  as.data.frame()\n\nbeta_2_xts &lt;- xts(\n  x = beta_2[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_2$.index\n)\n\ndygraph(beta_2_xts, main = \"Forecast with 95% CI - Beta*0002\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_2 = forecast_tbl2 |&gt; \n  subset(Location == '0002' & SKU =='Gamma')\n \ngamma_2 = gamma_2$forecast |&gt;\n  as.data.frame()\n\ngamma_2_xts &lt;- xts(\n  x = gamma_2[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_2$.index\n)\n\ndygraph(gamma_2_xts, main = \"Forecast with 95% CI - Gamma*0002\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_3 = forecast_tbl2 |&gt; \n  subset(Location == '0003' & SKU =='Alpha')\n \nalpha_3 = alpha_3$forecast |&gt;\n  as.data.frame()\n\nalpha_3_xts &lt;- xts(\n  x = alpha_3[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_3$.index\n)\n\ndygraph(alpha_3_xts, main = \"Forecast with 95% CI - Alpha*0003\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_3 = forecast_tbl2 |&gt; \n  subset(Location == '0003' & SKU =='Beta')\n \nbeta_3 = beta_3$forecast |&gt;\n  as.data.frame()\n\nbeta_3_xts &lt;- xts(\n  x = beta_3[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_3$.index\n)\n\ndygraph(beta_3_xts, main = \"Forecast with 95% CI - Beta*0003\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_3 = forecast_tbl2 |&gt; \n  subset(Location == '0003' & SKU =='Gamma')\n \ngamma_3 = gamma_3$forecast |&gt;\n  as.data.frame()\n\ngamma_3_xts &lt;- xts(\n  x = gamma_3[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_3$.index\n)\n\ndygraph(gamma_3_xts, main = \"Forecast with 95% CI - Gamma*0003\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_4 = forecast_tbl2 |&gt; \n  subset(Location == '0004' & SKU =='Alpha')\n \nalpha_4 = alpha_4$forecast |&gt;\n  as.data.frame()\n\nalpha_4_xts &lt;- xts(\n  x = alpha_4[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_4$.index\n)\n\ndygraph(alpha_4_xts, main = \"Forecast with 95% CI - Alpha*0004\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_4 = forecast_tbl2 |&gt; \n  subset(Location == '0004' & SKU =='Beta')\n \nbeta_4 = beta_4$forecast |&gt;\n  as.data.frame()\n\nbeta_4_xts &lt;- xts(\n  x = beta_4[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_4$.index\n)\n\ndygraph(beta_4_xts, main = \"Forecast with 95% CI - Beta*0004\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_4 = forecast_tbl2 |&gt; \n  subset(Location == '0004' & SKU =='Gamma')\n \ngamma_4 = gamma_4$forecast |&gt;\n  as.data.frame()\n\ngamma_4_xts &lt;- xts(\n  x = gamma_4[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_4$.index\n)\n\ndygraph(gamma_4_xts, main = \"Forecast with 95% CI - Gamma*0004\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_5 = forecast_tbl2 |&gt; \n  subset(Location == '0005' & SKU =='Alpha')\n \nalpha_5 = alpha_5$forecast |&gt;\n  as.data.frame()\n\nalpha_5_xts &lt;- xts(\n  x = alpha_5[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_5$.index\n)\n\ndygraph(alpha_5_xts, main = \"Forecast with 95% CI - Alpha*0005\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_5 = forecast_tbl2 |&gt; \n  subset(Location == '0005' & SKU =='Beta')\n \nbeta_5 = beta_5$forecast |&gt;\n  as.data.frame()\n\nbeta_5_xts &lt;- xts(\n  x = beta_5[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_5$.index\n)\n\ndygraph(beta_5_xts, main = \"Forecast with 95% CI - Beta*0005\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_5 = forecast_tbl2 |&gt; \n  subset(Location == '0005' & SKU =='Gamma')\n \ngamma_5 = gamma_5$forecast |&gt;\n  as.data.frame()\n\ngamma_5_xts &lt;- xts(\n  x = gamma_5[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_5$.index\n)\n\ndygraph(gamma_5_xts, main = \"Forecast with 95% CI - Gamma*0005\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_6 = forecast_tbl2 |&gt; \n  subset(Location == '0006' & SKU =='Alpha')\n \nalpha_6 = alpha_6$forecast |&gt;\n  as.data.frame()\n\nalpha_6_xts &lt;- xts(\n  x = alpha_6[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_6$.index\n)\n\ndygraph(alpha_6_xts, main = \"Forecast with 95% CI - Alpha*0006\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_6 = forecast_tbl2 |&gt; \n  subset(Location == '0006' & SKU =='Beta')\n \nbeta_6 = beta_6$forecast |&gt;\n  as.data.frame()\n\nbeta_6_xts &lt;- xts(\n  x = beta_6[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_6$.index\n)\n\ndygraph(beta_6_xts, main = \"Forecast with 95% CI - Beta*0006\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_6 = forecast_tbl2 |&gt; \n  subset(Location == '0006' & SKU =='Gamma')\n \ngamma_6 = gamma_6$forecast |&gt;\n  as.data.frame()\n\ngamma_6_xts &lt;- xts(\n  x = gamma_6[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_6$.index\n)\n\ndygraph(gamma_6_xts, main = \"Forecast with 95% CI - Gamma*0006\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_7 = forecast_tbl2 |&gt; \n  subset(Location == '0007' & SKU =='Alpha')\n \nalpha_7 = alpha_7$forecast |&gt;\n  as.data.frame()\n\nalpha_7_xts &lt;- xts(\n  x = alpha_7[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_7$.index\n)\n\ndygraph(alpha_7_xts, main = \"Forecast with 95% CI - Alpha*0007\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_7 = forecast_tbl2 |&gt; \n  subset(Location == '0007' & SKU =='Beta')\n \nbeta_7 = beta_7$forecast |&gt;\n  as.data.frame()\n\nbeta_7_xts &lt;- xts(\n  x = beta_7[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_7$.index\n)\n\ndygraph(beta_7_xts, main = \"Forecast with 95% CI - Beta*0007\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_7 = forecast_tbl2 |&gt; \n  subset(Location == '0007' & SKU =='Gamma')\n \ngamma_7 = gamma_7$forecast |&gt;\n  as.data.frame()\n\ngamma_7_xts &lt;- xts(\n  x = gamma_7[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_7$.index\n)\n\ndygraph(gamma_7_xts, main = \"Forecast with 95% CI - Gamma*0007\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_8 = forecast_tbl2 |&gt; \n  subset(Location == '0008' & SKU =='Alpha')\n \nalpha_8 = alpha_8$forecast |&gt;\n  as.data.frame()\n\nalpha_8_xts &lt;- xts(\n  x = alpha_8[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_8$.index\n)\n\ndygraph(alpha_8_xts, main = \"Forecast with 95% CI - Alpha*0008\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_8 = forecast_tbl2 |&gt; \n  subset(Location == '0008' & SKU =='Beta')\n \nbeta_8 = beta_8$forecast |&gt;\n  as.data.frame()\n\nbeta_8_xts &lt;- xts(\n  x = beta_8[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_8$.index\n)\n\ndygraph(beta_8_xts, main = \"Forecast with 95% CI - Beta*0008\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_8 = forecast_tbl2 |&gt; \n  subset(Location == '0008' & SKU =='Gamma')\n \ngamma_8 = gamma_8$forecast |&gt;\n  as.data.frame()\n\ngamma_8_xts &lt;- xts(\n  x = gamma_8[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_8$.index\n)\n\ndygraph(gamma_8_xts, main = \"Forecast with 95% CI - Gamma*0008\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_9 = forecast_tbl2 |&gt; \n  subset(Location == '0009' & SKU =='Alpha')\n \nalpha_9 = alpha_9$forecast |&gt;\n  as.data.frame()\n\nalpha_9_xts &lt;- xts(\n  x = alpha_9[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_9$.index\n)\n\ndygraph(alpha_9_xts, main = \"Forecast with 95% CI - Alpha*0009\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_9 = forecast_tbl2 |&gt; \n  subset(Location == '0009' & SKU =='Beta')\n \nbeta_9 = beta_9$forecast |&gt;\n  as.data.frame()\n\nbeta_9_xts &lt;- xts(\n  x = beta_9[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_9$.index\n)\n\ndygraph(beta_9_xts, main = \"Forecast with 95% CI - Beta*0009\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_9 = forecast_tbl2 |&gt; \n  subset(Location == '0009' & SKU =='Gamma')\n \ngamma_9 = gamma_9$forecast |&gt;\n  as.data.frame()\n\ngamma_9_xts &lt;- xts(\n  x = gamma_9[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_9$.index\n)\n\ndygraph(gamma_9_xts, main = \"Forecast with 95% CI - Gamma*0009\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nalpha_10 = forecast_tbl2 |&gt; \n  subset(Location == '0010' & SKU =='Alpha')\n \nalpha_10 = alpha_10$forecast |&gt;\n  as.data.frame()\n\nalpha_10_xts &lt;- xts(\n  x = alpha_10[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = alpha_10$.index\n)\n\ndygraph(alpha_10_xts, main = \"Forecast with 95% CI - Alpha*0010\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector() \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\nbeta_10 = forecast_tbl2 |&gt; \n  subset(Location == '0010' & SKU =='Beta')\n \nbeta_10 = beta_10$forecast |&gt;\n  as.data.frame()\n\nbeta_10_xts &lt;- xts(\n  x = beta_10[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = beta_10$.index\n)\n\ndygraph(beta_10_xts, main = \"Forecast with 95% CI - Beta*0010\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\nCode\nlibrary(xts)\nlibrary(dygraphs)\n\ngamma_10 = forecast_tbl2 |&gt; \n  subset(Location == '0010' & SKU =='Gamma')\n \ngamma_10 = gamma_10$forecast |&gt;\n  as.data.frame()\n\ngamma_10_xts &lt;- xts(\n  x = gamma_10[, c(\".value\", \".conf_lo\", \".conf_hi\")],\n  order.by = gamma_10$.index\n)\n\ndygraph(gamma_10_xts, main = \"Forecast with 95% CI - Gamma*0010\") |&gt;\n  dySeries(c(\".conf_lo\",\".value\", \".conf_hi\")) |&gt;\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize   = 2,\n    strokeWidth = 2\n  ) |&gt;\n  dyAxis(\"y\", label = \"Demand\") |&gt;\n  dyRangeSelector()   \n\n\n\n\n\n\n\n\n\n\n\n4. Takeaways & Limitations\n\nLow‐volume, intermittent series\n\nMost SKU×Location series average only a handful of units sold per month, with many zero‐demand months.\nSimple ARIMA/ETS/Prophet models struggled on the lowest‐volume, most intermittent series (high MAE/RMSE relative to mean demand).\nIf we cant to keep the SKU x Location level we could explore or time series forecasting methods which are designed for forecasting series with many zeros (Crostom for exemple.).Otherwise we can forecast at a higher level (Product Family or Region) and after that allocate down to SKU*Location by historical Sales.\n\nStock‐out bias\n\nOur data capture only fulfilled sales, not actual demand. When inventory is exhausted, recorded sales drop to zero, even if customer demand remained.\n\nInferring “Implied Demand”\n\nBy incorporating inventory or production capacity data, we can estimate the unobserved portion of demand . Forecasting this implied demand rather than raw sales should yield more accurate predictions of true customer need."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Demand Forecasting",
    "section": "",
    "text": "This project showcases my technical and analytical skills through the completion of a comprehensive data science assessment provided by IVADO Labs. It required performing exploratory data analysis (EDA) and developing predictive models to generate demand forecasts for use in an inventory management solution.\nLeveraging my primary proficiency in R, I conducted a detailed EDA and built an initial forecasting model using established time-series techniques. To demonstrate flexibility and broaden the solution’s applicability, I then implemented an alternative forecasting approach in Python, allowing for a direct comparison between two differents forecasting models with their pros and cons.\n\n\n\nExploratory Data Analysis (EDA)\nData Preparation and Modeling"
  },
  {
    "objectID": "index.html#key-objectives",
    "href": "index.html#key-objectives",
    "title": "Demand Forecasting",
    "section": "",
    "text": "Exploratory Data Analysis (EDA)\nData Preparation and Modeling"
  },
  {
    "objectID": "index.html#time-series-forecast-with-r",
    "href": "index.html#time-series-forecast-with-r",
    "title": "Demand Forecasting",
    "section": "Time-Series Forecast with R",
    "text": "Time-Series Forecast with R\nSee full code and explanation here.\nOn this first try, I forecast demand at the SKU × Location level for the following reasons:\n\nOperational relevance\n\nInventory and replenishment decisions are made at the store (Location) and item (SKU) level. Precise SKU × Location forecasts help minimize stock-outs and overstock.\n\nCaptures local demand patterns\n\nEach Location has its own seasonality, promotions, and customer mix.\n\nSupports hierarchical consistency\n\nA bottom-up SKU × Location forecast can be aggregated to Region or Family level, ensuring that high-level dashboards always roll up correctly from the detailed forecasts driving operations.\n\nManages noise vs. scale\n\nWith only 3 SKUs and 10 locations (30 series total), it’s a manageable modeling effort. We get detailed forecasts without an unmanageable number of models.\n\nFlexibility for enhancements, we can easily:\n\nTest different algorithms (ARIMA, ETS, Prophet) on each series and still reconcile them back up\n\n\nBy forecasting at the SKU × Location level, we ensure the most actionable, accurate, and consistent demand plans, while retaining the ability to roll up to any higher aggregation needed for reporting or strategic planning.\nI obtained moderate accuracy, explained by the following factors:\n\nLow-volume, intermittent series\n\nMost SKU × Location series average only a handful of units sold per month, with many zero-demand months.\n\nSimple ARIMA/ETS/Prophet models struggled on the lowest-volume, most intermittent series (high MAE/RMSE relative to mean demand).\n\nIf we want to stay at the SKU × Location level, we could explore time-series methods designed for intermittent series (e.g., Croston’s method). Otherwise, we can forecast at a higher level (Product Family or Region) and then allocate down to SKU × Location based on historical sales.\n\nStock-out bias\n\nOur sales data capture only fulfilled demand. When inventory is exhausted, recorded sales drop to zero, even if customer demand remained.\n\nInferring Implied Demand\n\nBy incorporating inventory or production data, we can estimate the unobserved portion of demand. Forecasting this implied demand rather than raw sales should yield more accurate predictions of true customer need."
  },
  {
    "objectID": "index.html#machine-learning-forecasting-with-lightgbm-in-python",
    "href": "index.html#machine-learning-forecasting-with-lightgbm-in-python",
    "title": "Demand Forecasting",
    "section": "Machine Learning Forecasting with LightGBM in Python",
    "text": "Machine Learning Forecasting with LightGBM in Python\nSee full code and explanation here.\nBuilding on the SKU × Location experiments, I applied a LightGBM model at the Product Family × Region Type level. This higher aggregation led to better accuracy and the following insights:\n\nAggregation helps\n\nPooling many SKU × Location series into a handful of family × region series stabilizes the demand signal and gives the model more to learn from. The average RMSE across the six series was ~11.5 units.\n\nMissing drivers\n\nWithout inventory or promotion data (implied demand features), the model cannot distinguish zeros from true zero demand vs. stock-outs.\n\nFeature enhancements could include:\n\nAdding calendar effects (holidays, seasonality flags)\n\nIntroducing exogenous variables (price, promotions, weather)\n\nEngineering higher-order lags or rolling windows"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m Thibault Senegas, a self-taught programmer and expert in R and Shiny. As a Lead Data Scientist and Chief Product Owner at quantum simulations*, I lead the creation of AI and Data solutions for companies, sports teams, and geographical regions. My focus is on merging cutting-edge data science with real-world applications.\nI hold an M.Sc. in International Business from HEC Montréal and a Master’s degree in Engineering from INP/ENSIACET in Toulouse, France. Before joining quantum simulations*, I honed my skills as a Data Scientist at IBM in their Advanced Analytics practice.\nI currently reside in Montréal, QC, with my wife and two sons.\nMy Resume"
  },
  {
    "objectID": "eda_R.html",
    "href": "eda_R.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Below you will find an EDA of the data provided."
  },
  {
    "objectID": "eda_R.html#quantity-sold-per-month-per-store-all-products-combined",
    "href": "eda_R.html#quantity-sold-per-month-per-store-all-products-combined",
    "title": "Exploratory Data Analysis",
    "section": "5.1 Quantity Sold per month per store (all products combined)",
    "text": "5.1 Quantity Sold per month per store (all products combined)\n\n\nCode\ndf_hc_qty &lt;- df_monthly |&gt; \n    group_by(month, `Location ID`) |&gt; \n    mutate(\n        quantity = sum(quantity)\n    ) |&gt; \n    ungroup() |&gt; \n    select(month, `Location ID`, quantity) |&gt; \n    unique() |&gt; \n    mutate(month_ts = datetime_to_timestamp(month)) \n\nhc_loc &lt;- highchart(type = \"stock\") |&gt; \n  hc_title(text = \"Monthly Quantity by Location\") |&gt; \n  hc_xAxis(type = \"datetime\")\n\nfor(loc in unique(df_hc_qty$`Location ID`)) {\n  tmp &lt;- df_hc_qty |&gt; \n   filter(`Location ID` == loc)\n  hc_loc &lt;- hc_loc |&gt; \n    hc_add_series(\n      data = list_parse2(tibble(x = tmp$month_ts, y = tmp$quantity)),\n      name = loc,\n      type = \"line\"\n    )\n}\n\nhc_loc |&gt; \n  hc_legend(enabled = TRUE, align = \"center\", layout = \"horizontal\", verticalAlign = \"bottom\")"
  },
  {
    "objectID": "eda_R.html#quantity-sold-per-month-per-region-type-all-products-combined",
    "href": "eda_R.html#quantity-sold-per-month-per-region-type-all-products-combined",
    "title": "Exploratory Data Analysis",
    "section": "5.2 Quantity Sold per month per Region type (all products combined)",
    "text": "5.2 Quantity Sold per month per Region type (all products combined)\n\n\nCode\ndf_hc_qty &lt;- df_monthly |&gt; \n    group_by(month, `Region Type`) |&gt; \n    mutate(\n        quantity = sum(quantity)\n    ) |&gt; \n    ungroup() |&gt; \n    select(month, `Region Type`, quantity) |&gt; \n    unique() |&gt; \n    mutate(month_ts = datetime_to_timestamp(month)) \n\nhc_loc &lt;- highchart(type = \"stock\") |&gt; \n  hc_title(text = \"Monthly Quantity by Region Type\") |&gt; \n  hc_xAxis(type = \"datetime\")\n\nfor(loc in unique(df_hc_qty$`Region Type`)) {\n  tmp &lt;- df_hc_qty |&gt; \n   filter(`Region Type` == loc)\n  hc_loc &lt;- hc_loc |&gt; \n    hc_add_series(\n      data = list_parse2(tibble(x = tmp$month_ts, y = tmp$quantity)),\n      name = loc,\n      type = \"line\"\n    )\n}\n\nhc_loc |&gt; \n  hc_legend(enabled = TRUE, align = \"center\", layout = \"horizontal\", verticalAlign = \"bottom\")"
  },
  {
    "objectID": "eda_R.html#quantity-sold-per-month-per-surface-area-all-products-combined",
    "href": "eda_R.html#quantity-sold-per-month-per-surface-area-all-products-combined",
    "title": "Exploratory Data Analysis",
    "section": "5.3 Quantity Sold per month per Surface area (all products combined)",
    "text": "5.3 Quantity Sold per month per Surface area (all products combined)\n\n\nCode\ndf_hc_qty &lt;- df_monthly |&gt; \n    group_by(month, `Surface Area`) |&gt; \n    mutate(\n        quantity = sum(quantity)\n    ) |&gt; \n    ungroup() |&gt; \n    select(month, `Surface Area`, quantity) |&gt; \n    unique() |&gt; \n    mutate(month_ts = datetime_to_timestamp(month)) \n\nhc_loc &lt;- highchart(type = \"stock\") |&gt; \n  hc_title(text = \"Monthly Quantity by locations' surface area\") |&gt; \n  hc_xAxis(type = \"datetime\")\n\nfor(loc in unique(df_hc_qty$`Surface Area`)) {\n  tmp &lt;- df_hc_qty |&gt; \n   filter(`Surface Area` == loc)\n  hc_loc &lt;- hc_loc |&gt; \n    hc_add_series(\n      data = list_parse2(tibble(x = tmp$month_ts, y = tmp$quantity)),\n      name = loc,\n      type = \"line\"\n    )\n}\n\nhc_loc |&gt; \n  hc_legend(enabled = TRUE, align = \"center\", layout = \"horizontal\", verticalAlign = \"bottom\")"
  },
  {
    "objectID": "eda_R.html#quantity-sold-per-month-per-product-family",
    "href": "eda_R.html#quantity-sold-per-month-per-product-family",
    "title": "Exploratory Data Analysis",
    "section": "5.3 Quantity Sold per month per Product Family",
    "text": "5.3 Quantity Sold per month per Product Family\n\n\nCode\ndf_hc_qty &lt;- df_monthly |&gt; \n    group_by(month, `Product Family`) |&gt; \n    mutate(\n        quantity = sum(quantity)\n    ) |&gt; \n    ungroup() |&gt; \n    select(month, `Product Family`, quantity) |&gt; \n    unique() |&gt; \n    mutate(month_ts = datetime_to_timestamp(month)) \n\nhc_loc &lt;- highchart(type = \"stock\") |&gt; \n  hc_title(text = \"Monthly Quantity by Product Family\") |&gt; \n  hc_xAxis(type = \"datetime\")\n\nfor(loc in unique(df_hc_qty$`Product Family`)) {\n  tmp &lt;- df_hc_qty |&gt; \n   filter(`Product Family` == loc)\n  hc_loc &lt;- hc_loc |&gt; \n    hc_add_series(\n      data = list_parse2(tibble(x = tmp$month_ts, y = tmp$quantity)),\n      name = loc,\n      type = \"line\"\n    )\n}\n\nhc_loc |&gt; \n  hc_legend(enabled = TRUE, align = \"center\", layout = \"horizontal\", verticalAlign = \"bottom\")"
  },
  {
    "objectID": "eda_R.html#promotionbundling-effects-on-quantity-sold",
    "href": "eda_R.html#promotionbundling-effects-on-quantity-sold",
    "title": "Exploratory Data Analysis",
    "section": "5.4 Promotion/Bundling effects on quantity sold",
    "text": "5.4 Promotion/Bundling effects on quantity sold\n\n\nCode\n# 1. Aggregate at month × SKU\ndf_sku &lt;- df_monthly %&gt;%\n  group_by(month, SKU) %&gt;%\n  summarise(\n    quantity  = sum(quantity),\n    avg_price = weighted.mean(unit_price),\n    .groups = \"drop\"\n  )\n\n# 2. Compute metrics per SKU\nresults &lt;- df_sku %&gt;%\n  split(.$SKU) %&gt;%                          # list of data.frames by SKU\n  map_dfr(function(dat){\n    # correlation test\n    ct   &lt;- cor.test(dat$avg_price, dat$quantity)\n    # linear regression\n    lm1  &lt;- lm(quantity ~ avg_price, data = dat)\n    # log‐log regression (only positive obs)\n    dat_pos &lt;- filter(dat, quantity &gt; 0, avg_price &gt; 0)\n    elast &lt;- if(nrow(dat_pos) &gt; 1) {\n      coef(lm(log(quantity) ~ log(avg_price), data = dat_pos))[\"log(avg_price)\"]\n    } else {\n      NA_real_\n    }\n    # assemble\n    tibble(\n      SKU           = dat$SKU[1],\n      Pearson_r     = as.numeric(ct$estimate),\n      p_value       = ct$p.value,\n      Linear_slope  = coef(lm1)[\"avg_price\"],\n      Elasticity    = elast\n    )\n  })\n\n# 3. Inspect\nprint(results)\n\n\n# A tibble: 3 × 5\n  SKU   Pearson_r p_value Linear_slope Elasticity\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Alpha     0.253  0.107         2.61       3.04 \n2 Beta      0.133  0.358         0.265      0.833\n3 Gamma     0.240  0.0933        1.78       0.828\n\n\nHere’s the summary of how unit price relates to quantity for each SKU:\n\nAlpha:\n\nPearson r ≈ 0.253 (p ≈ 0.107) – a modest positive association, not statistically significant at 5%.\nLinear slope ≈ 2.61 (units sold per CAD) – suggests higher prices for Alpha months coincide with slightly higher quantities sold (perhaps signalling premium perception or bundle effects).\nElasticity ≈ 3.04 – implies a 1 % increase in price is associated with ~3 % increase in quantity (counter-intuitive, but note significance is low and sample size small).\n\nBeta:\n\nPearson r ≈ 0.133 (p ≈ 0.358) – a weak positive correlation, not significant.\nLinear slope ≈ 0.26, elasticity ≈ 0.80 – effects are very small and statistically inconclusive.\n\nGamma:\n\nPearson r ≈ 0.240 (p ≈ 0.093) – again a mild positive link, borderline non-significant.\nLinear slope ≈ 1.78, elasticity ≈ 0.83.\n\n\nI concluded that none of the SKUs exhibit a clear inverse price–quantity relationship; instead, I observed modest positive correlations, which I believe are driven by promotions, bundling, or other confounding factors rather than true demand elasticity."
  },
  {
    "objectID": "modeling_Python.html",
    "href": "modeling_Python.html",
    "title": "Demand Forecasting Model with Python",
    "section": "",
    "text": "In our initial experiments we applied classical time-series methods (ARIMA, ETS, Prophet) directly at the SKU × Location granularity. While these methods are well-suited to long, dense series, we obtained only moderate forecasting accuracy for most SKU × Location series. Two key issues likely explain this:\n\nLow and intermittent sales per SKU × Location lead to noisy series with many zero-sales months.\nInventory censoring: without on-hand inventory data, our models see zeros when stock-outs occur—even if latent demand remained.\n\nBecause we currently lack the inventory feed needed to reconstruct implied demand, we’ll pivot to a machine-learning demand-forecasting framework:\n\nAlgorithm choice: we’ll use a gradient-boosted tree model (LightGBM) for its efficiency, native support for categorical features, and strong performance on sparse/heterogeneous data.\nAggregation level: by rolling up to Product‐Family × Region-Type and engineering calendar and lagged features, we can stabilize the target and give the model richer cross-series information.\n\n\n1. Data Prepation\n\nimport pandas as pd\n\n# 1. Load your monthly SKU×Location series\ndf = pd.read_csv('./data/df_monthly.csv', parse_dates=['month'])\n\ndf = df.rename(columns={\n    'month': 'date',\n    'Product Family': 'product_family' ,\n    'Region Type': 'region_type' ,\n    'Surface Area': 'surface_area'\n})\n\ndf_agg = (\n    df\n    .groupby(['date', 'product_family', 'region_type'], as_index=False)\n    ['quantity']\n    .sum()\n    .rename(columns={'qty': 'quantity' })\n)\n\n\n\n2. LightGBM Model\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\nimport pandas as pd\n\n# We'll store per-series results here\nresults = []\nmodels  = {}\n\n# Loop over each of the 6 series\nfor (pf, rt), grp in df_agg.groupby(['product_family', 'region_type']):\n    # 1. Sort by date and copy\n    df_grp = grp.sort_values('date').reset_index(drop=True).copy()\n    \n    avg_qty = grp['quantity'].mean()\n\n    # 2. Feature engineering\n    df_grp['year']  = df_grp['date'].dt.year\n    df_grp['month'] = df_grp['date'].dt.month\n    # lags\n    for lag in [1, 2, 3]:\n        df_grp[f'lag_{lag}'] = df_grp['quantity'].shift(lag)\n    # rolling 3-month mean\n    df_grp['rolling_3'] = df_grp['quantity'].shift(1).rolling(3).mean()\n    \n    # 3. Drop rows with any NA (due to lags/rolling)\n    df_feat = df_grp.dropna().reset_index(drop=True)\n    \n    # 4. Split features / target\n    X = df_feat[['year','month','lag_1','lag_2','lag_3','rolling_3']]\n    y = df_feat['quantity']\n    \n    # 5. Time-aware train/test split (80% train)\n    split_idx = int(len(df_feat)*0.8)\n    X_train, X_test = X.iloc[:split_idx],    X.iloc[split_idx:]\n    y_train, y_test = y.iloc[:split_idx],    y.iloc[split_idx:]\n    \n    # 6. Prepare LightGBM datasets\n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_test,  label=y_test)\n    \n    # 7. Model hyperparameters\n    params = {\n        'objective':      'regression',\n        'metric':         'rmse',\n        'learning_rate':  0.05,\n        'num_leaves':     31,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq':     5,\n        'verbose':         -1\n    }\n    \n    # 8. Train with early stopping\n    model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=1000,\n        valid_sets=[valid_data],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0)      # silence the built-in logging\n        ]\n    )\n    \n    # 9. Predict and evaluate\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    mse  = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    avg_qty = np.mean(y_train)\n    \n    # 10. Store results\n    results.append({\n        'product_family': pf,\n        'region_type':    rt,\n        'test_rmse':      round(rmse, 2),\n        'test_mae':       round(mae, 2),\n        'avg_qty':        round(avg_qty, 2)\n    })\n    models[(pf, rt)] = model\n\n# Convert results to a DataFrame for easy viewing\nresults_df = pd.DataFrame(results)\nprint(results_df[['product_family','region_type','test_rmse','test_mae','avg_qty']])\n\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 17.2467\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 12.1171\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 8.58222\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 9.23605\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 6.5924\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1] valid_0's rmse: 15.0765\n  product_family region_type  test_rmse  test_mae  avg_qty\n0          X1-G3       Rural      17.25     11.20    26.76\n1          X1-G3  Semi-Urban      12.12      8.30    16.86\n2          X1-G3       Urban       8.58      7.70    35.69\n3          X1-S7       Rural       9.24      8.44    27.84\n4          X1-S7  Semi-Urban       6.59      5.51    16.54\n5          X1-S7       Urban      15.08     11.10    36.00\n\n\n\n\nTakeaways & Limitations\n\nAggregation helps: pooling many SKU×Location series into a handful of family×region series stabilizes the demand signal and gives the model more to learn from. The average RMSE accross the 6 series is ~11.5 units.\nMissing drivers: without inventory or promotion data (implied demand features), the model can’t distinguish zeros from true zero demand vs. stock-outs.\nFeature enhancements:\n\nAdd calendar effects (holidays, seasonality flags)\nIntroduce exogenous variables (price, promotions, weather)\nEngineer higher-order lags or rolling windows"
  }
]