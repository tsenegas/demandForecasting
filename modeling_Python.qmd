---
title: "Demand Forecasting Model with Python"
author: "Th. Senegas"
date: "2025-05-26"
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
execute:
  warning: false
---

In our initial experiments we applied classical time-series methods (ARIMA, ETS, Prophet) directly at the SKU × Location granularity. While these methods are well-suited to long, dense series, we obtained only moderate forecasting accuracy for most SKU × Location series. Two key issues likely explain this:

- **Low and intermittent sales** per SKU × Location lead to noisy series with many zero-sales months.

- **Inventory censoring:** without on-hand inventory data, our models see zeros when stock-outs occur—even if latent demand remained.

Because we currently lack the inventory feed needed to reconstruct implied demand, we’ll pivot to a machine-learning demand-forecasting framework:

- Algorithm choice: we’ll use a gradient-boosted tree model (LightGBM) for its efficiency, native support for categorical features, and strong performance on sparse/heterogeneous data.

- Aggregation level: by rolling up to Product‐Family × Region-Type and engineering calendar and lagged features, we can stabilize the target and give the model richer cross-series information.

# 1. Data Prepation

```{python}
import pandas as pd

# 1. Load monthly data savec previously
df = pd.read_csv('./data/df_monthly.csv', parse_dates=['month'])

df = df.rename(columns={
    'month': 'date',
    'Product Family': 'product_family' ,
    'Region Type': 'region_type' ,
    'Surface Area': 'surface_area'
})

df_agg = (
    df
    .groupby(['date', 'product_family', 'region_type'], as_index=False)
    ['quantity']
    .sum()
    .rename(columns={'qty': 'quantity' })
)

```

# 2. LightGBM Model

```{python}
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

results = []
models  = {}

# Loop over each of the 6 series
for (pf, rt), grp in df_agg.groupby(['product_family', 'region_type']):
    # 1. Sort by date and copy
    df_grp = grp.sort_values('date').reset_index(drop=True).copy()
    
    avg_qty = grp['quantity'].mean()

    # 2. Feature engineering
    df_grp['year']  = df_grp['date'].dt.year
    df_grp['month'] = df_grp['date'].dt.month
    # lags
    for lag in [1, 2, 3]:
        df_grp[f'lag_{lag}'] = df_grp['quantity'].shift(lag)
    # rolling 3-month mean
    df_grp['rolling_3'] = df_grp['quantity'].shift(1).rolling(3).mean()
    
    # 3. Drop rows with any NA (due to lags/rolling)
    df_feat = df_grp.dropna().reset_index(drop=True)
    
    # 4. Split features / target
    X = df_feat[['year','month','lag_1','lag_2','lag_3','rolling_3']]
    y = df_feat['quantity']
    
    # 5. Time-aware train/test split (80% train)
    split_idx = int(len(df_feat)*0.8)
    X_train, X_test = X.iloc[:split_idx],    X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx],    y.iloc[split_idx:]
    
    # 6. Prepare LightGBM datasets
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test,  label=y_test)
    
    # 7. Model hyperparameters
    params = {
        'objective':      'regression',
        'metric':         'rmse',
        'learning_rate':  0.05,
        'num_leaves':     31,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq':     5,
        'verbose':         -1
    }
    
    # 8. Train with early stopping
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[valid_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=0)  
        ]
    )
    
    # 9. Predict and evaluate
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)
    mse  = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    avg_qty = np.mean(y_train)
    
    # 10. Store results
    results.append({
        'product_family': pf,
        'region_type':    rt,
        'test_rmse':      round(rmse, 2),
        'test_mae':       round(mae, 2),
        'avg_qty':        round(avg_qty, 2)
    })
    models[(pf, rt)] = model

# Convert results to a df
results_df = pd.DataFrame(results)
print(results_df[['product_family','region_type','test_rmse','test_mae','avg_qty']])

```

# Takeaways & Limitations

- **Aggregation helps**: pooling many SKU×Location series into a handful of family×region series stabilizes the demand signal and gives the model more to learn from. The average RMSE accross the 6 series is ~11.5 units.

- **Missing drivers**: without inventory or promotion data (implied demand features), the model can’t distinguish zeros from true zero demand vs. stock-outs.

- **Feature enhancements**:

    - Add calendar effects (holidays, seasonality flags)
    - Introduce exogenous variables (price, promotions, weather)
    - Engineer higher-order lags or rolling windows