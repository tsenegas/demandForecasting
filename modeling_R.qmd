---
title: "Demand Forecasting Model with R"
author: "Th. Senegas"
date: Sys.Date()
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
execute:
  warning: false
---

I chose to forecast at the SKU × Location level for several reasons:

-   **Operational relevance**

    -   Inventory and replenishment decisions are made at the store (location) and item (SKU) level. By forecasting precisely where and what to stock, we can minimize stock-outs and overstock.

-   **Captures local demand patterns**

    -   Each location has its own seasonality, promotions, and customer mix.

-   **Supports hierarchical consistency**

    -   A bottom-up SKU×Location forecast can be aggregated automatically to Region or Family level views. That means our high-level dashboards always roll up correctly from the detailed forecasts driving operations.

-   **Manages noise vs. scale**

    -   With only 3 SKUs and 10 locations (30 series total), it’s a manageable modeling effort. We get the benefit of detailed forecasts without an unmanageable number of models.

-   **Flexibility for enhancements**

-   We can easily swap in covariates (e.g. price, promotions, local events) per SKU×Location, or test different algorithms (ARIMA, ETS, Prophet) on each series and still reconcile them back up.

By forecasting at the SKU × Location level, we ensure the most actionable, accurate, and consistent demand plans, while retaining the ability to roll up to any higher aggregation needed for reporting or strategic planning.

# 1. Data Prepation

```{r }
#| code-fold: true

# 0. Packages
library(tidyverse)
library(lubridate)
library(modeltime)   # forecasting bridges tidymodels + prophet + ARIMA + ETS
library(timetk)      # time‐series toolkit
library(rsample)     # for resampling
library(parsnip)     # model specification
library(recipes)     # feature engineering
library(workflows)   # bundling recipe + model
library(yardstick)   # metrics
library(tsibble) 

# 1) Read & prep
df_monthly <- read_csv("./data/df_monthly.csv") |>
  select(month, Location = `Location ID`, SKU, quantity) |>
  group_by(month, Location, SKU) |>
  summarise(quantity = sum(quantity), .groups = "drop") |>
  mutate(month = yearmonth(month)) |>
  as_tsibble(key = c(Location, SKU), index = month) |>
  fill_gaps(quantity = 0L)

df <- df_monthly |>
  select(month, Location, SKU, quantity) |>
  mutate(
    ds = yearmonth(month) |>  as.Date(),  # convert to Date at month‐start
    y  = quantity
  ) |>
  select(ds, y, Location, SKU)

# 2) Nest by series
nested_tbl <- df |> 
  group_by(Location, SKU) |> 
  nest() |> 
  ungroup()

```

# 2. Model Training & Evaluation
```{r}
#| code-fold: true

set.seed(123)

# 3) Define a function to build a modeltime table per series
make_models <- function(data_tbl) {

  # a. Split train / test (last 6 months hold-out)
  splits <- initial_time_split(data_tbl, prop = (nrow(data_tbl) - 6) / nrow(data_tbl))

  # b. Recipe (no extra features right now)
  rec <- recipe(y ~ ds, data = training(splits))
  
  # c. Model specs
  arima_spec <- arima_reg()      |>  set_engine("auto_arima")
  ets_spec   <- exp_smoothing()  |>  set_engine("ets")
  prop_spec  <- prophet_reg()    |>  set_engine("prophet")

  # d. Workflows
  wf_arima <- workflow() |>  add_model(arima_spec) |>  add_recipe(rec)
  wf_ets   <- workflow() |>  add_model(ets_spec)   |>  add_recipe(rec)
  wf_prop  <- workflow() |>  add_model(prop_spec)  |>  add_recipe(rec)

  # e. Modeltime table
  modeltime_table(
    arima = fit(wf_arima,  data = training(splits)),
    ets   = fit(wf_ets,    data = training(splits)),
    prop  = fit(wf_prop,   data = training(splits))
  ) |> 
    # f. Calibrate (generate forecasts on the test set)
    modeltime_calibrate(new_data = testing(splits)) |> 
    # g. Compute accuracy metrics
    modeltime_accuracy() |> 
    # h. Also embed the calibration object for later forecasting
    left_join(
      modeltime_table(
        arima = fit(wf_arima,  data = training(splits)),
        ets   = fit(wf_ets,    data = training(splits)),
        prop  = fit(wf_prop,   data = training(splits))
      ) |>  modeltime_calibrate(new_data = testing(splits)) |>  mutate(.model_desc = names(.model_id)),
      by = ".model_id"
    )
}

# 4) Apply to each series
results_tbl <- nested_tbl |> 
  mutate(
    metrics = map(data, make_models)
  ) |> 
  select(Location, SKU, metrics) |> 
  unnest(metrics)

 best_models <- results_tbl |> 
  group_by(Location, SKU) |> 
  slice_min(rmse, n = 1) |> 
  select(Location, SKU, .model_desc, mae, rmse, mase) |> 
  distinct(Location, SKU, .keep_all = TRUE)

# 5) Inspect accuracy
results_tbl |> 
  select(Location, SKU, .model_desc, rmse, mae, mape) |> 
  arrange(Location, SKU, rmse) |> 
  print(n = Inf)
```

# 3. Final Forecasting
```{r}
#| code-fold: true

# 6) Re-nest for final forecasting
forecast_tbl <- nested_tbl |> 
  left_join(
    results_tbl |> 
      group_by(Location, SKU) |> 
      slice_min(rmse, n = 1) |> 
      select(Location, SKU, .model_id),
    by = c("Location","SKU")
  ) |> 
  distinct(Location, SKU, .keep_all = TRUE) |> 
  mutate(
    # 1) build a 3-row modeltime_table for each series, and tag rows with .model_id = 1,2,3
    model_tbl = map(data, function(df) {
      rec      <- recipe(y ~ ds, data = df)
      wf_arima <- workflow() |> 
                    add_model(arima_reg() |>  set_engine("auto_arima")) |> 
                    add_recipe(rec)
      wf_ets   <- workflow() |> 
                    add_model(exp_smoothing() |>  set_engine("ets")) |> 
                    add_recipe(rec)
      wf_prop  <- workflow() |> 
                    add_model(prophet_reg() |>  set_engine("prophet")) |> 
                    add_recipe(rec)

      modeltime_table(
        arima = fit(wf_arima, df),
        ets   = fit(wf_ets,   df),
        prop  = fit(wf_prop,  df)
      ) #|> 
      #rowid_to_column(var = ".model_id")
    }),

    # 2) pick the chosen row by .model_id
    model = map2(model_tbl, .model_id, ~ slice(.x, .y)),

    # 3) refit that model on _all_ the data, then forecast 6 months out
    final_model = map2(model, data, ~ modeltime_refit(.x, data = .y))#,
    #forecast    = map(final_model,     ~ modeltime_forecast(.x, h = "6 months"))
  )

forecast_tbl2 <- forecast_tbl |> 
  mutate(
    # Re-calibrate each final_model on its own data
    calib_full = map2(final_model, data, ~ modeltime_calibrate(.x, new_data = .y)),

    # Now you *can* use `h = "6 months"`, but you MUST also give it `actual_data`
    forecast = map2(
      calib_full, data,
      ~ modeltime_forecast(.x,
          h           = "6 months",
          actual_data = .y
      )
    )
  )
```

# 4. Takeaways & Limitations